<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
    content="RayRoPE: Projective Ray Positional Encoding for Multi-view Attention">
  <meta name="keywords" content="positional encoding, multi-view transformer, 3D vision, RoPE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RayRoPE: Projective Ray Positional Encoding for Multi-view Attention</title>
  <link rel="icon" type="image/png" href="./assets/rayrope_logo.png">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro|Caveat">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <script src="https://cdn.plot.ly/plotly-3.3.0.min.js"></script>
  
  <style>
    :root {
      --primary-color: #3273dc;
      --secondary-color: #c9a227;
      --dark-gray: #555;
      --light-gray: #666;
    }

    body {
      font-family: 'Noto Sans', sans-serif;
    }

    .publication-title {
      font-family: 'Google Sans', sans-serif;
    }

    .tldr-section {
      padding: 2rem 0;
    }

    .tldr-card {
      background: white;
      border-radius: 12px;
      padding: 2rem;
      box-shadow: 0 4px 16px rgba(0,0,0,0.08);
    }

    .abstract-block {
      background: white;
      border-radius: 12px;
      padding: 1.25rem 1.5rem;
      box-shadow: 0 4px 16px rgba(0,0,0,0.08);
      border: 1px solid #eee;
    }

    .abstract-summary {
      cursor: pointer;
      font-weight: 600;
      font-size: 1.25rem;
      color: var(--primary-color);
      list-style: none;
      outline: none;
      text-align: center;
    }

    .abstract-summary::-webkit-details-marker {
      display: none;
    }

    .abstract-summary::after {
      content: '▸';
      margin-left: 0.5rem;
      transition: transform 0.2s ease;
      display: inline-block;
    }

    details[open] .abstract-summary::after {
      transform: rotate(90deg);
    }

    .abstract-content {
      margin-top: 0.75rem;
      color: var(--light-gray);
      line-height: 1.6;
      text-align: justify;
      text-justify: inter-word;
    }

    .tldr-pill {
      background: var(--primary-color);
      color: white;
      padding: 0.25rem 0.75rem;
      border-radius: 20px;
      font-weight: 600;
      font-size: 0.85rem;
      margin-right: 0.75rem;
    }

    .tldr-heading {
      display: flex;
      align-items: center;
      margin-bottom: 1rem;
    }

    .tldr-list {
      list-style: none;
      list-style-type: none;
      padding-left: 0;
      margin-left: 0;
    }

    .tldr-list li {
      list-style: none;
      padding: 0.5rem 0;
      padding-left: 1.5rem;
      position: relative;
      cursor: pointer;
      transition: background-color 0.2s;
    }

    .tldr-list li:hover {
      background-color: #f5f5f5;
      border-radius: 4px;
    }

    .tldr-list li::before {
      content: "•";
      position: absolute;
      left: 0;
      color: var(--primary-color);
      font-size: 1.2rem;
      line-height: 1;
      animation: wave-blink 2s ease-in-out infinite;
    }

    @keyframes wave-blink {
      0%, 100% { 
        opacity: 0.4;
        transform: scale(1);
      }
      25% { 
        opacity: 1;
        transform: scale(1.2);
      }
      50% { 
        opacity: 0.6;
        transform: scale(1.1);
      }
      75% { 
        opacity: 1;
        transform: scale(1.3);
      }
    }

    .section-subtitle {
      color: var(--light-gray);
      margin-top: -0.5rem;
      margin-bottom: 1rem;
    }

    .property-card {
      background: white;
      border-radius: 8px;
      padding: 1rem;
      box-shadow: 0 2px 8px rgba(0,0,0,0.08);
      text-align: center;
      height: 100%;
    }

    .property-card h4 {
      color: var(--primary-color);
      margin-top: 0.5rem;
    }

    .comparison-table {
      width: 100%;
      border-collapse: collapse;
      margin: 1.5rem 0;
    }

    .comparison-table th,
    .comparison-table td {
      padding: 0.75rem 1rem;
      text-align: center;
      border: 1px solid #ddd;
    }

    .comparison-table th {
      background: #a8d4f0;
      color: #333;
    }

    .comparison-table tr:nth-child(even) {
      background: #f9f9f9;
    }

    .check-mark {
      color: #22c55e;
      font-size: 1.25rem;
    }

    .cross-mark {
      color: #ef4444;
      font-size: 1.25rem;
    }

    /* Image placeholder styles */
    .image-placeholder {
      background: linear-gradient(135deg, #e0e0e0 0%, #f5f5f5 100%);
      border: 2px dashed #ccc;
      border-radius: 8px;
      display: flex;
      align-items: center;
      justify-content: center;
      color: #999;
      font-size: 0.9rem;
      min-height: 200px;
    }

    .image-placeholder.tall {
      min-height: 400px;
    }

    .image-placeholder.medium {
      min-height: 300px;
    }

    /* Video comparison slider */
    .video-comparison-container {
      position: relative;
      width: 100%;
      max-width: 400px;
      margin: 0 auto;
      overflow: hidden;
      border-radius: 8px;
      background: #000;
    }

    .video-comparison-wrapper {
      position: relative;
      width: 100%;
      aspect-ratio: 1/1;
    }

    .video-comparison-wrapper video {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      object-fit: cover;
    }

    .video-left {
      clip-path: inset(0 50% 0 0);
    }

    .video-right {
      clip-path: inset(0 0 0 50%);
    }

    .comparison-slider {
      position: absolute;
      top: 0;
      left: 50%;
      width: 4px;
      height: 100%;
      background: white;
      cursor: ew-resize;
      z-index: 10;
      transform: translateX(-50%);
    }

    .comparison-slider::before {
      content: "⟨ ⟩";
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      background: white;
      padding: 8px 4px;
      border-radius: 4px;
      font-size: 14px;
      color: #333;
      white-space: nowrap;
    }

    .video-label {
      position: absolute;
      bottom: 10px;
      padding: 4px 12px;
      background: rgba(0,0,0,0.7);
      color: white;
      border-radius: 4px;
      font-size: 0.85rem;
      z-index: 5;
    }

    .video-label.left {
      left: 10px;
    }

    .video-label.right {
      right: 10px;
    }

    .method-selector {
      display: flex;
      gap: 0.5rem;
      flex-wrap: wrap;
      justify-content: center;
      margin-bottom: 1rem;
    }

    .method-selector button {
      padding: 0.5rem 1rem;
      border: 2px solid #ddd;
      background: white;
      border-radius: 4px;
      cursor: pointer;
      transition: all 0.2s;
    }

    .method-selector button:hover {
      border-color: var(--primary-color);
    }

    .method-selector button.active {
      background: var(--primary-color);
      color: white;
      border-color: var(--primary-color);
    }

    /* Scene selector / thumbnail scroll */
    .scene-selector {
      display: flex;
      gap: 0.75rem;
      overflow-x: auto;
      padding: 1.5rem 0;
      scroll-behavior: smooth;
      scrollbar-width: thin;
      justify-content: flex-start;
    }

    .scene-selector::before,
    .scene-selector::after {
      content: '';
      flex: 1;
    }

    .scene-selector::-webkit-scrollbar {
      height: 6px;
    }

    .scene-selector::-webkit-scrollbar-track {
      background: #f1f1f1;
      border-radius: 10px;
    }

    .scene-selector::-webkit-scrollbar-thumb {
      background: #888;
      border-radius: 10px;
    }

    .scene-item {
      flex: 0 0 auto;
      display: flex;
      gap: 0.5rem;
      padding: 0.5rem;
      border-radius: 8px;
      cursor: pointer;
      border: 2px solid transparent;
      transition: all 0.3s;
      background: white;
    }

    .scene-item:hover {
      border-color: var(--primary-color);
      box-shadow: 0 2px 8px rgba(50, 115, 220, 0.2);
    }

    .scene-item.active {
      border-color: var(--primary-color);
      box-shadow: 0 4px 12px rgba(50, 115, 220, 0.3);
      background: rgba(50, 115, 220, 0.05);
    }

    .scene-item .ref-image {
      width: 60px;
      height: 60px;
      border-radius: 4px;
      object-fit: cover;
      border: 1px solid #ddd;
    }

    /* Two-row figure grid */
    .two-row-figure {
      display: grid;
      grid-template-columns: repeat(4, 1fr);
      gap: 1rem;
    }

    .two-row-figure .figure-item {
      text-align: center;
    }

    .two-row-figure .figure-item img,
    .two-row-figure .figure-item .image-placeholder {
      width: 100%;
      aspect-ratio: 1;
      object-fit: contain;
    }

    .two-row-figure .figure-item p {
      margin-top: 0.5rem;
      font-size: 0.9rem;
      color: var(--light-gray);
    }

    /* Attention demo styles */
    .attention-demo {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 2rem;
      margin-top: 1.5rem;
    }

    .slider-group {
      margin: 1rem 0;
    }

    .slider-group label {
      display: block;
      margin-bottom: 0.25rem;
      font-weight: 500;
    }

    .slider-group input[type="range"] {
      width: 100%;
    }

    .heatmap-placeholder {
      width: 100%;
      aspect-ratio: 1;
      background: linear-gradient(135deg, #3273dc22 0%, #c9a22722 100%);
      border: 2px dashed #ccc;
      border-radius: 8px;
      display: flex;
      align-items: center;
      justify-content: center;
      color: #999;
    }

    /* Depth comparison styles */
    .depth-comparison {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 1rem;
      margin-top: 1.5rem;
    }

    .depth-item {
      text-align: center;
    }

    .depth-item img,
    .depth-item .image-placeholder {
      width: 100%;
      aspect-ratio: 4/3;
      object-fit: cover;
      border-radius: 8px;
    }

    .depth-item p {
      margin-top: 0.5rem;
      font-size: 0.85rem;
      color: var(--light-gray);
    }

    /* Stereo point cloud grid */
    .stereo-pointcloud-grid {
      display: grid;
      grid-template-columns: repeat(2, 1fr);
      gap: 0.75rem;
      margin-top: 1rem;
    }

    .stereo-pointcloud-item {
      text-align: center;
    }

    .stereo-pointcloud-item .pointcloud-container {
      width: 100%;
      height: 280px;
      border-radius: 8px;
      border: 1px solid #ddd;
      background: #fafafa;
    }

    .stereo-pointcloud-item p {
      margin-top: 0.25rem;
      font-size: 1rem;
      font-weight: 500;
      color: var(--dark-gray);
    }

    @media (max-width: 768px) {
      .stereo-pointcloud-grid {
        grid-template-columns: 1fr;
      }
      .stereo-pointcloud-item .pointcloud-container {
        height: 240px;
      }
    }

    /* Custom video player controls */
    .custom-video-container {
      max-width: 1200px;
      width: 100%;
      margin: 0 auto;
    }

    .custom-video-container video {
      width: 100%;
      border-radius: 8px;
      display: block;
    }

    .video-controls {
      display: flex;
      align-items: center;
      gap: 10px;
      padding: 8px 0;
      margin-top: 8px;
    }

    .play-pause-btn {
      background: none;
      border: none;
      color: #666;
      font-size: 0.9rem;
      cursor: pointer;
      padding: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      transition: color 0.2s;
    }

    .play-pause-btn:hover {
      color: var(--primary-color);
    }

    .progress-container {
      flex: 1;
      height: 4px;
      background: #e0e0e0;
      border-radius: 2px;
      cursor: pointer;
      position: relative;
    }

    .progress-bar {
      height: 100%;
      background: var(--primary-color);
      border-radius: 2px;
      width: 0%;
      position: relative;
      pointer-events: none;
    }

    .progress-bar::after {
      content: '';
      position: absolute;
      right: -5px;
      top: 50%;
      transform: translateY(-50%);
      width: 10px;
      height: 10px;
      background: var(--primary-color);
      border-radius: 50%;
      opacity: 0;
      transition: opacity 0.2s;
    }

    .progress-container:hover .progress-bar::after,
    .progress-container.dragging .progress-bar::after {
      opacity: 1;
    }

    .time-display {
      color: #888;
      font-size: 0.75rem;
      font-family: monospace;
      min-width: 80px;
      text-align: right;
    }

    /* Responsive */
    @media (max-width: 768px) {
      .two-row-figure {
        grid-template-columns: repeat(2, 1fr);
      }

      .attention-demo {
        grid-template-columns: 1fr;
      }
    }
  </style>
</head>

<body>

<!-- Hero Section -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="is-flex is-align-items-center is-justify-content-center" style="gap:0.75rem; margin-bottom:0.25rem;">
            <img src="./assets/rayrope_logo.png" alt="RayRoPE icon" style="height: 3.5rem;" />
           
            <h1 class="title is-1 publication-title" style="margin:0; letter-spacing: 0.02em;">
              <img src="./assets/rayrope_title.png" alt="RayRoPE" style="height: 3.5rem; margin-top: 1.2rem;" />
            </h1>
          </div>
          <h3 class="title is-3 publication-title">
            <!-- <span style="color: #3273dc; font-weight: 700;">Geometric-Grounded Positional Encoding</span>
            <span style="font-weight: 400; color: #666;">for</span>
            <span style="color: #c9a227; font-style: italic; font-weight: 500;">Multi-View Transformers</span> -->
            Projective  Ray Positional Encoding for Multi-view Attention
        </h3>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://lucas-707.github.io/">Yu Wu</a><sup>1</sup>&nbsp;&nbsp;</span>
            <span class="author-block"><a href="https://msjeon.me/">Minsik Jeon</a><sup>1</sup>&nbsp;&nbsp;</span>
            <span class="author-block"><a href="https://rick-chang.github.io/">Jen-Hao Rick Chang</a><sup>2</sup>&nbsp;&nbsp;</span>
            <span class="author-block"><a href="https://www.onceltuzel.net/">Oncel Tuzel</a><sup>2</sup>&nbsp;&nbsp;</span>
            <span class="author-block"><a href="https://shubhtuls.github.io/">Shubham Tulsiani</a><sup>1</sup>&nbsp;&nbsp;</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup> Carnegie Mellon University&nbsp;&nbsp;&nbsp;</span>
            <span class="author-block"><sup>2</sup> Apple</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2601.15275v1" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/Lucas-707/RayRoPE" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract Section -->
<section class="section" id="abstract">
  <div class="container is-max-desktop">
    <details class="abstract-block">
      <summary class="abstract-summary">Abstract</summary>
      <div class="abstract-content">
        <p>
          <!-- TODO: Replace with the paper abstract. -->
        We study positional encodings for multi-view transformers that process tokens from a set of posed input images, 
        and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, 
        and can be adaptive to the geometry of the underlying scene. 
        We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, 
        and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays 
        but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. 
        To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. 
        Lastly, as the 'predicted' 3D point along a ray may not be precise, 
        RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. 
        We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and 
        show that it consistently improves over alternate position encoding schemes 
        (e.g. 15% relative improvement on LPIPS in Co3D). 
        We also show that RayRoPE can seamlessly incorporate RGB-D input, 
        resulting in even larger gains over alternatives that cannot positionally encode this information.           
        </p>
      </div>
    </details>
  </div>
</section>

<!-- Section 1: Problem -->
<section class="section" id="problem">
  <div class="container is-max-desktop">
    <h2 class="title is-4">How should we design positional encoding for multi-view transformers?</h2>
    
    <div class="content has-text-justified" style="margin-bottom: 1.5rem;">
      <p>
        We live in a <strong>3D world</strong>, and the positional encoding of transformers should be <strong>3D-aware</strong> as well. 
        In this project, we propose <span style="color: #3273dc; font-weight: 700;">RayRoPE</span>, a novel relative
        positional encoding mechanism designed for multi-view attention.
      </p>
    </div>

    <!-- Two-row figure: Row 1 shows 1D/2D/3D/Multi-view models, Row 2 shows corresponding RoPE variants -->
    <div class="content has-text-centered">
      <div class="custom-video-container">
        <video id="teaser-video" autoplay muted playsinline preload="metadata" data-src="./assets/web-teaser.mp4">
          <source type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-controls">
          <button class="play-pause-btn" id="teaser-play-pause-btn">
            <i class="fas fa-pause"></i>
          </button>
          <div class="progress-container" id="teaser-progress-container">
            <div class="progress-bar" id="teaser-progress-bar"></div>
          </div>
          <span class="time-display" id="teaser-time-display">0:00 / 0:00</span>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Section 2: Properties -->
<section class="section" id="properties">
  <div class="container is-max-desktop">
    <h2 class="title is-4">What makes a good positional encoding in multi-view?</h2>
    <div class="content has-text-justified" style="margin-bottom: 2rem;">
      <p>
        We argue that an ideal positional encoding for multi-view attention should satisfy <strong>four desirable properties</strong> illustrated below:
      </p>
    </div>

    <!-- Four desirable properties figure: (a) SE(3) Invariance, (b) Uniqueness, (c) Geometric Adaptiveness, (d) Multi-frequency Similarity -->
    <div class="content has-text-centered" style="margin-bottom: 2rem;">
      <!-- Placeholder - remove when image is ready -->
      <!-- <div class="image-placeholder" style="min-height: 250px; max-width: 100%; margin: 0 auto;">
        <span>Figure: Four desirable properties (a) SE(3) Invariance, (b) Uniqueness, (c) Geometric Adaptiveness, (d) Multi-frequency</span>
      </div> -->
      
      <img src="./assets/intro_new.png" alt="Four desirable properties for 3D positional encoding" style="width: 100%; border-radius: 8px;" />
     
    </div>

    <!-- Comparison table -->
    <table class="comparison-table">
      <thead>
        <tr>
          <th>Method</th>
          <th>SE(3) Invariance</th>
          <th>Uniqueness</th>
          <th>Geometric Adaptiveness</th>
          <th>Multi-frequency</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Plücker Raymap</strong></td>
          <td><span class="cross-mark">✗</span></td>
          <td><span class="check-mark">✓</span></td>
          <td><span class="cross-mark">✗</span></td>
          <td><span class="cross-mark">✗</span></td>
        </tr>
        <tr>
          <td><strong>RoPE on Raymap</strong></td>
          <td><span class="cross-mark">✗</span></td>
          <td><span class="check-mark">✓</span></td>
          <td><span class="cross-mark">✗</span></td>
          <td><span class="check-mark">✓</span></td>
        </tr>
        <tr>
          <td><strong>GTA, PRoPE</strong></td>
          <td><span class="check-mark">✓</span></td>
          <td><span class="cross-mark">✗</span></td>
          <td><span class="cross-mark">✗</span></td>
          <td><span class="cross-mark">✗</span> (only for patch indices)</td>
        </tr>
        <tr style="background: linear-gradient(90deg, #3273dc11 0%, #c9a22711 100%);">
          <td><strong style="color: #3273dc;">RayRoPE (Ours)</strong></td>
          <td><span class="check-mark">✓</span></td>
          <td><span class="check-mark">✓</span></td>
          <td><span class="check-mark">✓</span></td>
          <td><span class="check-mark">✓</span></td>
        </tr>
      </tbody>
    </table>
  </div>
</section>

<!-- Section 3: Method-->
<section class="section" id="method-rope">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Method: RayRoPE Encodings</h2>
    <div class="content has-text-justified">
    </div>
  </div>
  
  <!-- Video container outside max-desktop to be wider -->
  <div class="container is-max-widescreen">
    <div class="content has-text-centered">
      <div class="custom-video-container">
        <video id="method-video" autoplay muted playsinline preload="metadata" data-src="./assets/method_animation_copy.mp4">
          <source type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-controls">
          <button class="play-pause-btn" id="play-pause-btn">
            <i class="fas fa-pause"></i>
          </button>
          <div class="progress-container" id="progress-container">
            <div class="progress-bar" id="progress-bar"></div>
          </div>
          <span class="time-display" id="time-display">0:00 / 0:00</span>
        </div>
      </div>
    </div>
  </div>
  
  <div class="container is-max-desktop">
    <!-- Key Insights -->
    <div class="content has-text-justified" style="margin-top: 2rem;">
      <ul class="tldr-list">
        <li><strong style="color: var(--primary-color);">Insight 1:</strong>
          We can parametrize patch positions via the associated ray(s), using a predicted point along the ray to allow adaptation to scene geometry
        <li><strong style="color: var(--primary-color);">Insight 2:</strong> 
          Projecting rays into query camera's frame ensure independence from world coordinates.
        <li><strong style="color: var(--primary-color);">Insight 3:</strong> 
          Modeling uncertainties in depth via expected RoPE produces more stable encodings.
        </ul>
    </div>
  </div>
</section>

<!-- Section 5: (Optional) Applying RayRoPE to Attention -->
<!-- <section class="section" id="attention-figure">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Applying RayRoPE to Attention</h2>
    <div class="content has-text-centered">      
      <img src="./assets/apply-attn.png" alt="RayRoPE Applied to Multi-View Attention" style="max-width: 600px; width: 100%; border-radius: 8px;" />
    </div>
  </div>
</section> -->



<!-- Section 7: Novel View Synthesis Comparison -->
<section class="section" id="nvs-comparison">
  <div class="container is-max-desktop">
    <h2 class="title is-4">RayRoPE Improves Novel View Synthesis</h2>
    
    <div class="content has-text-justified">
      <p>
        We train the LVSM model with different positional encodings and compare their results below. 
      </p>
    </div>

    <!-- Method selectors -->
    <div style="display: flex; gap: 2rem; justify-content: center; margin-bottom: 1rem;">
      <div>
        <p class="has-text-centered is-size-7" style="margin-bottom: 0.5rem;"><strong>Left Video:</strong></p>
        <div class="method-selector" id="left-method-selector">
          <button data-method="gt">Ground Truth</button>
          <button data-method="plucker">Plücker</button>
          <button data-method="gta">GTA</button>
          <button data-method="prope" class="active">PRoPE</button>
          <button data-method="rayrope">RayRoPE</button>
        </div>
      </div>
      <div>
        <p class="has-text-centered is-size-7" style="margin-bottom: 0.5rem;"><strong>Right Video:</strong></p>
        <div class="method-selector" id="right-method-selector">
          <button data-method="gt">Ground Truth</button>
          <button data-method="plucker">Plücker</button>
          <button data-method="gta">GTA</button>
          <button data-method="prope">PRoPE</button>
          <button data-method="rayrope" class="active">RayRoPE</button>
        </div>
      </div>
    </div>

    <!-- Video comparison slider -->
    <div class="video-comparison-container" id="nvs-video-container">
      <div class="video-comparison-wrapper">
        <video class="video-left" id="video-left" autoplay muted loop playsinline preload="metadata">
          <source type="video/mp4">
        </video>
        <video class="video-right" id="video-right" autoplay muted loop playsinline preload="metadata">
          <source type="video/mp4">
        </video>
        <div class="comparison-slider" id="comparison-slider"></div>
        <div class="video-label left" id="label-left">PRoPE</div>
        <div class="video-label right" id="label-right">RayRoPE</div>
      </div>
    </div>

    <!-- Scene selector -->
    <p class="has-text-centered is-size-12" style="margin-top: 2rem; margin-bottom: -1.5rem;"><strong>Select Scene (Input Views):</strong></p>
    <div class="scene-selector" id="scene-selector">
      <!-- Scene 0 -->
      <div class="scene-item active" data-scene="0" data-dataset="co3d_seen">
        <img src="./assets/nvs-visual/co3d_seen_0_ref0.png" alt="Scene 0 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_0_ref1.png" alt="Scene 0 Ref 1" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_0_ref2.png" alt="Scene 0 Ref 2" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_0_ref3.png" alt="Scene 0 Ref 3" class="ref-image">
      </div>
      <!-- Scene 1 -->
      <div class="scene-item" data-scene="1" data-dataset="co3d_seen">
        <img src="./assets/nvs-visual/co3d_seen_1_ref0.png" alt="Scene 1 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_1_ref1.png" alt="Scene 1 Ref 1" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_1_ref2.png" alt="Scene 1 Ref 2" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_1_ref3.png" alt="Scene 1 Ref 3" class="ref-image">
      </div>
      <!-- Scene 2 -->
      <div class="scene-item" data-scene="2" data-dataset="co3d_seen">
        <img src="./assets/nvs-visual/co3d_seen_2_ref0.png" alt="Scene 2 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_2_ref1.png" alt="Scene 2 Ref 1" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_2_ref2.png" alt="Scene 2 Ref 2" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_2_ref3.png" alt="Scene 2 Ref 3" class="ref-image">
      </div>
      <!-- Scene 3 -->
      <div class="scene-item" data-scene="3" data-dataset="co3d_seen">
        <img src="./assets/nvs-visual/co3d_seen_3_ref0.png" alt="Scene 3 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_3_ref1.png" alt="Scene 3 Ref 1" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_3_ref2.png" alt="Scene 3 Ref 2" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_3_ref3.png" alt="Scene 3 Ref 3" class="ref-image">
      </div>
      <!-- Scene 4 -->
      <!-- <div class="scene-item" data-scene="4" data-dataset="co3d_seen">
        <img src="./assets/nvs-visual/co3d_seen_4_ref0.png" alt="Scene 4 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_4_ref1.png" alt="Scene 4 Ref 1" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_4_ref2.png" alt="Scene 4 Ref 2" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_4_ref3.png" alt="Scene 4 Ref 3" class="ref-image">
      </div> -->
      <!-- Scene 5 -->
      <div class="scene-item" data-scene="5" data-dataset="co3d_seen">
        <img src="./assets/nvs-visual/co3d_seen_5_ref0.png" alt="Scene 5 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_5_ref1.png" alt="Scene 5 Ref 1" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_5_ref2.png" alt="Scene 5 Ref 2" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_5_ref3.png" alt="Scene 5 Ref 3" class="ref-image">
      </div>
      <!-- Scene 6 -->
      <div class="scene-item" data-scene="6" data-dataset="co3d_seen">
        <img src="./assets/nvs-visual/co3d_seen_6_ref0.png" alt="Scene 6 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_6_ref1.png" alt="Scene 6 Ref 1" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_6_ref2.png" alt="Scene 6 Ref 2" class="ref-image">
        <img src="./assets/nvs-visual/co3d_seen_6_ref3.png" alt="Scene 6 Ref 3" class="ref-image">
      </div>
      <!-- re10k Scene 0 -->
      <div class="scene-item" data-scene="0" data-dataset="re10k">
        <img src="./assets/nvs-visual/re10k_0_ref0.png" alt="re10k 0 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/re10k_0_ref1.png" alt="re10k 0 Ref 1" class="ref-image">
      </div>
      <!-- re10k Scene 1 -->
      <div class="scene-item" data-scene="1" data-dataset="re10k">
        <img src="./assets/nvs-visual/re10k_1_ref0.png" alt="re10k 1 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/re10k_1_ref1.png" alt="re10k 1 Ref 1" class="ref-image">
      </div>
      <!-- re10k Scene 2 -->
      <div class="scene-item" data-scene="2" data-dataset="re10k">
        <img src="./assets/nvs-visual/re10k_2_ref0.png" alt="re10k 2 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/re10k_2_ref1.png" alt="re10k 2 Ref 1" class="ref-image">
      </div>
      <!-- re10k Scene 3 -->
      <div class="scene-item" data-scene="3" data-dataset="re10k">
        <img src="./assets/nvs-visual/re10k_3_ref0.png" alt="re10k 3 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/re10k_3_ref1.png" alt="re10k 3 Ref 1" class="ref-image">
      </div>
      <!-- re10k Scene 4 -->
      <div class="scene-item" data-scene="4" data-dataset="re10k">
        <img src="./assets/nvs-visual/re10k_4_ref0.png" alt="re10k 4 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/re10k_4_ref1.png" alt="re10k 4 Ref 1" class="ref-image">
      </div>
      <!-- re10k Scene 5 -->
      <div class="scene-item" data-scene="5" data-dataset="re10k">
        <img src="./assets/nvs-visual/re10k_5_ref0.png" alt="re10k 5 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/re10k_5_ref1.png" alt="re10k 5 Ref 1" class="ref-image">
      </div>
      <!-- re10k Scene 6 -->
      <!-- <div class="scene-item" data-scene="6" data-dataset="re10k">
        <img src="./assets/nvs-visual/re10k_6_ref0.png" alt="re10k 6 Ref 0" class="ref-image">
        <img src="./assets/nvs-visual/re10k_6_ref1.png" alt="re10k 6 Ref 1" class="ref-image">
      </div> -->
    </div>
  </div>
</section>

<!-- Section 8: Stereo Depth Estimation -->
<section class="section" id="depth-estimation">
  <div class="container is-max-desktop">
    <h2 class="title is-4">RayRoPE Improves Stereo Depth Estimation</h2>
    <div class="content has-text-justified">
      <p>
        We evaluate the stereo depth estimation performance with the Unimatch model. The 3D point clouds below show the predicted depth for each method:
      </p>
    </div>

    <!-- Point cloud visualization grid -->
    <div class="stereo-pointcloud-grid">
      <div class="stereo-pointcloud-item">
        <div id="pointcloud-gt" class="pointcloud-container"></div>
        <p>Ground Truth</p>
      </div>
      <div class="stereo-pointcloud-item">
        <div id="pointcloud-unimatch" class="pointcloud-container"></div>
        <p>UniMatch</p>
      </div>
      <div class="stereo-pointcloud-item">
        <div id="pointcloud-prope" class="pointcloud-container"></div>
        <p>PRoPE</p>
      </div>
      <div class="stereo-pointcloud-item">
        <div id="pointcloud-rayrope" class="pointcloud-container"></div>
        <p>RayRoPE (Ours)</p>
      </div>
    </div>

    <!-- Scene selector for stereo depth (below visuals) -->
    <p class="has-text-centered is-size-12" style="margin-top: 1.5rem; margin-bottom: -1.5rem;"><strong>Select Scene (Input Views):</strong></p>
    <div class="scene-selector" id="stereo-scene-selector">
      <div class="scene-item active" data-scene="0001">
        <img src="./assets/stereo/inputs/ref_0001.png" alt="Scene 0001 Ref" class="ref-image">
        <img src="./assets/stereo/inputs/tgt_0001.png" alt="Scene 0001 Target" class="ref-image">
      </div>
      <div class="scene-item" data-scene="0125">
        <img src="./assets/stereo/inputs/ref_0125.png" alt="Scene 0125 Ref" class="ref-image">
        <img src="./assets/stereo/inputs/tgt_0125.png" alt="Scene 0125 Target" class="ref-image">
      </div>
      <div class="scene-item" data-scene="0071">
        <img src="./assets/stereo/inputs/ref_0071.png" alt="Scene 0071 Ref" class="ref-image">
        <img src="./assets/stereo/inputs/tgt_0071.png" alt="Scene 0071 Target" class="ref-image">
      </div>
      <div class="scene-item" data-scene="0033">
        <img src="./assets/stereo/inputs/ref_0033.png" alt="Scene 0033 Ref" class="ref-image">
        <img src="./assets/stereo/inputs/tgt_0033.png" alt="Scene 0033 Target" class="ref-image">
      </div>
      <div class="scene-item" data-scene="0048">
        <img src="./assets/stereo/inputs/ref_0048.png" alt="Scene 0048 Ref" class="ref-image">
        <img src="./assets/stereo/inputs/tgt_0048.png" alt="Scene 0048 Target" class="ref-image">
      </div>
      <!-- <div class="scene-item" data-scene="0067">
        <img src="./assets/stereo/inputs/ref_0067.png" alt="Scene 0067 Ref" class="ref-image">
        <img src="./assets/stereo/inputs/tgt_0067.png" alt="Scene 0067 Target" class="ref-image">
      </div> -->
      
      <!-- <div class="scene-item" data-scene="0093">
        <img src="./assets/stereo/inputs/ref_0093.png" alt="Scene 0093 Ref" class="ref-image">
        <img src="./assets/stereo/inputs/tgt_0093.png" alt="Scene 0093 Target" class="ref-image">
      </div> -->
      
      <!-- <div class="scene-item" data-scene="0150">
        <img src="./assets/stereo/inputs/ref_0150.png" alt="Scene 0150 Ref" class="ref-image">
        <img src="./assets/stereo/inputs/tgt_0150.png" alt="Scene 0150 Target" class="ref-image">
      </div> -->
      <!-- <div class="scene-item" data-scene="0153">
        <img src="./assets/stereo/inputs/ref_0153.png" alt="Scene 0153 Ref" class="ref-image">
        <img src="./assets/stereo/inputs/tgt_0153.png" alt="Scene 0153 Target" class="ref-image">
      </div> -->
    </div>
  </div>
</section>

<!-- Section 9: Emergent Depth -->
<section class="section" id="emergent-depth">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Emergent Depth</h2>
    <div class="content has-text-justified">
      <p>
        RayRoPE predicts depth and uncertainties which are used to compute the positional encodings. 
        Even without depth supervision during training, resonable depth predictions emerge, especially 
        in the later layers. Depth prediction accuracy and the predicted uncertainties are inversely correlated.
      </p>
    </div>

    <!-- View selector for depth visualization -->
    <div class="method-selector" id="depth-view-selector" style="margin-bottom: 1.5rem;">
      <button data-view="visualization" class="active">Visualization</button>
      <button data-view="accuracy">Accuracy vs. Uncertainty</button>
    </div>

    <!-- Depth visualization container -->
    <div class="content has-text-centered" style="margin-top: 1rem; margin-bottom: 1rem;">
      <div style="max-width: 1000px; margin: 0 auto;">
        <img id="depth-pred-view" src="./assets/depth_visual_final/101_11750_20715_v0.png" alt="Emergent Depth Visualization" style="width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" />
        <img id="depth-error-view" src="./assets/depth_error.png" alt="Depth Accuracy Plot" style="max-width: 450px; width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); display: none; margin: 0 auto;" />
      </div>
    </div>

    <!-- Scene selector for emergent depth (below visuals) -->
    <p class="has-text-centered is-size-12" style="margin-top: 1.5rem; margin-bottom: -1.5rem;"><strong>Select Scene:</strong></p>
    <div class="scene-selector" id="emergent-depth-scene-selector">
      <div class="scene-item active" data-scene="101_11750_20715_v0">
        <img src="./assets/depth_visual_final/101_11750_20715_v0_input.png" alt="Scene 101" class="ref-image">
      </div>
      <div class="scene-item" data-scene="106_12662_23043_v0">
        <img src="./assets/depth_visual_final/106_12662_23043_v0_input.png" alt="Scene 106" class="ref-image">
      </div>
      <div class="scene-item" data-scene="108_12854_22710_v0">
        <img src="./assets/depth_visual_final/108_12854_22710_v0_input.png" alt="Scene 108" class="ref-image">
      </div>
      <div class="scene-item" data-scene="109_12949_23115_v0">
        <img src="./assets/depth_visual_final/109_12949_23115_v0_input.png" alt="Scene 109" class="ref-image">
      </div>
      <div class="scene-item" data-scene="110_13051_23361_v0">
        <img src="./assets/depth_visual_final/110_13051_23361_v0_input.png" alt="Scene 110" class="ref-image">
      </div>
    </div>
  </div>
</section>

<!-- Section 6: (Optional) Interactive Attention Demo -->
<!-- <section class="section" id="attention-demo">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Interactive Attention Similarity Demo</h2>
    <div class="content has-text-justified">
      <p>
        Explore how attention similarity changes with camera transformations. The query camera is fixed at identity,
        and you can control the key camera's position and rotation using the sliders below.
      </p>
    </div>
    
    <div class="attention-demo">
      <div>
        <h5 class="title is-6">Camera Positions (3D View)</h5>
        <div class="image-placeholder" style="aspect-ratio: 1; min-height: 300px;">
          <span>3D Visualization: Query & Key Cameras</span>
        </div>
        <div class="slider-group">
          <label>Translation X: <span id="tx-value">0</span></label>
          <input type="range" id="slider-tx" min="-5" max="5" value="0" step="0.1">
        </div>
        <div class="slider-group">
          <label>Translation Y: <span id="ty-value">0</span></label>
          <input type="range" id="slider-ty" min="-5" max="5" value="0" step="0.1">
        </div>
        <div class="slider-group">
          <label>Translation Z: <span id="tz-value">0</span></label>
          <input type="range" id="slider-tz" min="-5" max="5" value="0" step="0.1">
        </div>
        <div class="slider-group">
          <label>Rotation (Yaw): <span id="rot-value">0</span>°</label>
          <input type="range" id="slider-rot" min="-180" max="180" value="0" step="1">
        </div>
      </div>
      
      <div>
        <h5 class="title is-6">Attention Similarity Heatmap (Key View)</h5>
        <div class="heatmap-placeholder">
          <span>Attention Heatmap Visualization</span>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- Footer -->
<footer class="footer" style="padding: 1.5rem 0;">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p style="margin: 0.2rem 0;">
            This webpage template is adapted from 
            <a href="https://github.com/nerfies/nerfies.github.io" target="_blank" rel="noreferrer">Nerfies</a> 
            under a <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noreferrer">CC BY-SA 4.0</a> License.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
document.addEventListener('DOMContentLoaded', function() {
  // ========== Smooth Scroll ==========
  function smoothScroll(element) {
    const target = element.getAttribute('data-target');
    const targetElement = document.querySelector(target);
    if (targetElement) {
      targetElement.scrollIntoView({ behavior: 'smooth', block: 'start' });
    }
  }

  // Make tldr list items clickable
  document.querySelectorAll('.tldr-list li[data-target]').forEach(item => {
    item.addEventListener('click', () => smoothScroll(item));
  });

  // ========== Slider Demo Values ==========
  const sliders = ['tx', 'ty', 'tz', 'rot'];
  sliders.forEach(id => {
    const slider = document.getElementById(`slider-${id}`);
    const valueSpan = document.getElementById(`${id}-value`);
    if (slider && valueSpan) {
      slider.addEventListener('input', () => {
        valueSpan.textContent = slider.value;
      });
    }
  });

  // ========== Video Comparison Slider ==========
  const container = document.getElementById('nvs-video-container');
  const slider = document.getElementById('comparison-slider');
  const videoLeft = document.getElementById('video-left');
  const videoRight = document.getElementById('video-right');
  
  let isDragging = false;

  function updateSliderPosition(clientX) {
    const rect = container.getBoundingClientRect();
    let x = clientX - rect.left;
    x = Math.max(0, Math.min(x, rect.width));
    const percent = (x / rect.width) * 100;
    
    slider.style.left = `${percent}%`;
    videoLeft.style.clipPath = `inset(0 ${100 - percent}% 0 0)`;
    videoRight.style.clipPath = `inset(0 0 0 ${percent}%)`;
  }

  slider.addEventListener('mousedown', () => { isDragging = true; });
  document.addEventListener('mouseup', () => { isDragging = false; });
  document.addEventListener('mousemove', (e) => {
    if (isDragging) {
      updateSliderPosition(e.clientX);
    }
  });

  // Touch support
  slider.addEventListener('touchstart', () => { isDragging = true; });
  document.addEventListener('touchend', () => { isDragging = false; });
  document.addEventListener('touchmove', (e) => {
    if (isDragging && e.touches.length > 0) {
      updateSliderPosition(e.touches[0].clientX);
    }
  });

  // Sync videos
  videoLeft.addEventListener('play', () => {
    videoRight.currentTime = videoLeft.currentTime;
    videoRight.play();
  });
  videoLeft.addEventListener('pause', () => videoRight.pause());
  videoLeft.addEventListener('seeked', () => {
    videoRight.currentTime = videoLeft.currentTime;
  });

  // ========== Method Selectors ==========
  const methodNames = {
    'gt': 'Ground Truth',
    'plucker': 'Plücker',
    'gta': 'GTA',
    'prope': 'PRoPE',
    'rayrope': 'RayRoPE'
  };

  const methodFileNames = {
    'gt': 'gt',
    'plucker': 'Plucker_pred',
    'gta': 'GTA_pred',
    'prope': 'PRoPE_pred',
    'rayrope': 'RayRoPE_pred'
  };

  let currentScene = '0';
  let currentDataset = 'co3d_seen';
  let leftMethod = 'prope';
  let rightMethod = 'rayrope';
  let nvsEnabled = false;

  function updateVideos() {
    if (!nvsEnabled) return;
    // Update video sources based on current selections
    // Format: ./assets/nvs-visual/{dataset}_{scene}_{method}.mp4
    const leftSrc = `./assets/nvs-visual/${currentDataset}_${currentScene}_${methodFileNames[leftMethod]}.mp4`;
    const rightSrc = `./assets/nvs-visual/${currentDataset}_${currentScene}_${methodFileNames[rightMethod]}.mp4`;
    
    videoLeft.src = leftSrc;
    videoRight.src = rightSrc;
    
    // Update labels
    document.getElementById('label-left').textContent = methodNames[leftMethod];
    document.getElementById('label-right').textContent = methodNames[rightMethod];

    // Sync and play
    videoLeft.load();
    videoRight.load();
    
    Promise.all([
      videoLeft.play().catch(() => {}),
      videoRight.play().catch(() => {})
    ]);
  }

  function enableNvsVideos() {
    if (nvsEnabled) return;
    nvsEnabled = true;
    updateVideos();
  }

  // Left method selector
  document.querySelectorAll('#left-method-selector button').forEach(btn => {
    btn.addEventListener('click', () => {
      document.querySelectorAll('#left-method-selector button').forEach(b => b.classList.remove('active'));
      btn.classList.add('active');
      leftMethod = btn.dataset.method;
      updateVideos();
    });
  });

  // Right method selector
  document.querySelectorAll('#right-method-selector button').forEach(btn => {
    btn.addEventListener('click', () => {
      document.querySelectorAll('#right-method-selector button').forEach(b => b.classList.remove('active'));
      btn.classList.add('active');
      rightMethod = btn.dataset.method;
      updateVideos();
    });
  });

  // Scene selector
  document.querySelectorAll('#scene-selector .scene-item').forEach(item => {
    item.addEventListener('click', () => {
      document.querySelectorAll('#scene-selector .scene-item').forEach(t => t.classList.remove('active'));
      item.classList.add('active');
      currentScene = item.dataset.scene;
      currentDataset = item.dataset.dataset;
      updateVideos();
    });
  });

  const nvsContainer = document.getElementById('nvs-video-container');
  if (nvsContainer && 'IntersectionObserver' in window) {
    const observer = new IntersectionObserver((entries) => {
      entries.forEach((entry) => {
        if (entry.isIntersecting) {
          enableNvsVideos();
          observer.disconnect();
        }
      });
    }, { rootMargin: '200px 0px' });
    observer.observe(nvsContainer);
  } else {
    enableNvsVideos();
  }
});
</script>

<!-- Custom Video Player Controls -->
<script>
// Generic video player setup function
function setupVideoPlayer(videoId, playPauseBtnId, progressContainerId, progressBarId, timeDisplayId, options = {}) {
  const video = document.getElementById(videoId);
  const playPauseBtn = document.getElementById(playPauseBtnId);
  const progressContainer = document.getElementById(progressContainerId);
  const progressBar = document.getElementById(progressBarId);
  const timeDisplay = document.getElementById(timeDisplayId);

  if (!video || !playPauseBtn || !progressContainer || !progressBar || !timeDisplay) return;

  // Format time as M:SS
  function formatTime(seconds) {
    const mins = Math.floor(seconds / 60);
    const secs = Math.floor(seconds % 60);
    return `${mins}:${secs.toString().padStart(2, '0')}`;
  }

  // Update play/pause button icon
  function updatePlayPauseIcon() {
    const icon = playPauseBtn.querySelector('i');
    if (video.paused) {
      icon.classList.remove('fa-pause');
      icon.classList.add('fa-play');
    } else {
      icon.classList.remove('fa-play');
      icon.classList.add('fa-pause');
    }
  }

  // Update progress bar and time display
  function updateProgress() {
    if (video.duration) {
      const percent = (video.currentTime / video.duration) * 100;
      progressBar.style.width = percent + '%';
      timeDisplay.textContent = `${formatTime(video.currentTime)} / ${formatTime(video.duration)}`;
    }
  }

  // Play/Pause toggle
  playPauseBtn.addEventListener('click', function() {
    if (video.paused) {
      video.play();
    } else {
      video.pause();
    }
  });

  // Drag to seek
  let isDragging = false;

  function seek(e) {
    if (!video.duration) return;
    const rect = progressContainer.getBoundingClientRect();
    let percent = (e.clientX - rect.left) / rect.width;
    percent = Math.max(0, Math.min(1, percent));
    video.currentTime = percent * video.duration;
    updateProgress();
  }

  progressContainer.addEventListener('mousedown', function(e) {
    e.preventDefault();
    isDragging = true;
    progressContainer.classList.add('dragging');
    seek(e);
  });

  document.addEventListener('mousemove', function(e) {
    if (isDragging) {
      e.preventDefault();
      seek(e);
    }
  });

  document.addEventListener('mouseup', function() {
    if (isDragging) {
      isDragging = false;
      progressContainer.classList.remove('dragging');
    }
  });

  // Touch events for mobile
  function seekTouch(touch) {
    if (!video.duration) return;
    const rect = progressContainer.getBoundingClientRect();
    let percent = (touch.clientX - rect.left) / rect.width;
    percent = Math.max(0, Math.min(1, percent));
    video.currentTime = percent * video.duration;
    updateProgress();
  }

  progressContainer.addEventListener('touchstart', function(e) {
    isDragging = true;
    progressContainer.classList.add('dragging');
    const touch = e.touches[0];
    seekTouch(touch);
  }, { passive: true });

  document.addEventListener('touchmove', function(e) {
    if (isDragging) {
      const touch = e.touches[0];
      seekTouch(touch);
    }
  }, { passive: true });

  document.addEventListener('touchend', function() {
    if (isDragging) {
      isDragging = false;
      progressContainer.classList.remove('dragging');
    }
  });

  // Event listeners for video
  video.addEventListener('timeupdate', updateProgress);
  video.addEventListener('play', updatePlayPauseIcon);
  video.addEventListener('pause', updatePlayPauseIcon);
  video.addEventListener('loadedmetadata', function() {
    timeDisplay.textContent = `${formatTime(0)} / ${formatTime(video.duration)}`;
  });

  // Handle video end - stay at last frame
  if (options.pauseOnEnd) {
    video.addEventListener('ended', function() {
      video.pause();
      updatePlayPauseIcon();
    });
  }

  function ensureVideoLoaded() {
    const dataSrc = video.getAttribute('data-src');
    if (dataSrc && !video.getAttribute('src')) {
      video.setAttribute('src', dataSrc);
      video.load();
    }
  }

  // Initialize
  updatePlayPauseIcon();

  if ('IntersectionObserver' in window) {
    const observer = new IntersectionObserver((entries) => {
      entries.forEach((entry) => {
        if (entry.isIntersecting) {
          ensureVideoLoaded();
          observer.disconnect();
        }
      });
    }, { rootMargin: '200px 0px' });
    observer.observe(video);
  } else {
    ensureVideoLoaded();
  }
}

document.addEventListener('DOMContentLoaded', function() {
  // Setup teaser video (pause on end to stay at last frame)
  setupVideoPlayer('teaser-video', 'teaser-play-pause-btn', 'teaser-progress-container', 'teaser-progress-bar', 'teaser-time-display', { pauseOnEnd: true });

  // Setup method video
  setupVideoPlayer('method-video', 'play-pause-btn', 'progress-container', 'progress-bar', 'time-display', { pauseOnEnd: false });
});
</script>

<!-- Legacy Video Player Controls (kept for compatibility) -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  const video = document.getElementById('method-video');
  const playPauseBtn = document.getElementById('play-pause-btn');
  const progressContainer = document.getElementById('progress-container');
  const progressBar = document.getElementById('progress-bar');
  const timeDisplay = document.getElementById('time-display');

  if (!video || !playPauseBtn || !progressContainer || !progressBar || !timeDisplay) return;

  // Format time as M:SS
  function formatTime(seconds) {
    const mins = Math.floor(seconds / 60);
    const secs = Math.floor(seconds % 60);
    return `${mins}:${secs.toString().padStart(2, '0')}`;
  }

  // Update play/pause button icon
  function updatePlayPauseIcon() {
    const icon = playPauseBtn.querySelector('i');
    if (video.paused) {
      icon.classList.remove('fa-pause');
      icon.classList.add('fa-play');
    } else {
      icon.classList.remove('fa-play');
      icon.classList.add('fa-pause');
    }
  }

  // Update progress bar and time display
  function updateProgress() {
    if (video.duration) {
      const percent = (video.currentTime / video.duration) * 100;
      progressBar.style.width = percent + '%';
      timeDisplay.textContent = `${formatTime(video.currentTime)} / ${formatTime(video.duration)}`;
    }
  }

  // Play/Pause toggle
  playPauseBtn.addEventListener('click', function() {
    if (video.paused) {
      video.play();
    } else {
      video.pause();
    }
  });

  // Drag to seek
  let isDragging = false;

  function seek(e) {
    if (!video.duration) return;
    const rect = progressContainer.getBoundingClientRect();
    let percent = (e.clientX - rect.left) / rect.width;
    percent = Math.max(0, Math.min(1, percent));
    video.currentTime = percent * video.duration;
    updateProgress();
  }

  progressContainer.addEventListener('mousedown', function(e) {
    e.preventDefault();
    isDragging = true;
    progressContainer.classList.add('dragging');
    seek(e);
  });

  document.addEventListener('mousemove', function(e) {
    if (isDragging) {
      e.preventDefault();
      seek(e);
    }
  });

  document.addEventListener('mouseup', function() {
    if (isDragging) {
      isDragging = false;
      progressContainer.classList.remove('dragging');
    }
  });

  // Also handle touch events for mobile
  progressContainer.addEventListener('touchstart', function(e) {
    isDragging = true;
    progressContainer.classList.add('dragging');
    const touch = e.touches[0];
    seekTouch(touch);
  }, { passive: true });

  document.addEventListener('touchmove', function(e) {
    if (isDragging) {
      const touch = e.touches[0];
      seekTouch(touch);
    }
  }, { passive: true });

  document.addEventListener('touchend', function() {
    if (isDragging) {
      isDragging = false;
      progressContainer.classList.remove('dragging');
    }
  });

  function seekTouch(touch) {
    if (!video.duration) return;
    const rect = progressContainer.getBoundingClientRect();
    let percent = (touch.clientX - rect.left) / rect.width;
    percent = Math.max(0, Math.min(1, percent));
    video.currentTime = percent * video.duration;
    updateProgress();
  }

  // Event listeners for video
  video.addEventListener('timeupdate', updateProgress);
  video.addEventListener('play', updatePlayPauseIcon);
  video.addEventListener('pause', updatePlayPauseIcon);
  video.addEventListener('loadedmetadata', function() {
    timeDisplay.textContent = `${formatTime(0)} / ${formatTime(video.duration)}`;
  });

  function ensureMethodVideoLoaded() {
    const dataSrc = video.getAttribute('data-src');
    if (dataSrc && !video.getAttribute('src')) {
      video.setAttribute('src', dataSrc);
      video.load();
    }
  }

  // Initialize
  updatePlayPauseIcon();

  if ('IntersectionObserver' in window) {
    const observer = new IntersectionObserver((entries) => {
      entries.forEach((entry) => {
        if (entry.isIntersecting) {
          ensureMethodVideoLoaded();
          observer.disconnect();
        }
      });
    }, { rootMargin: '200px 0px' });
    observer.observe(video);
  } else {
    ensureMethodVideoLoaded();
  }
});
</script>

<!-- Stereo Point Cloud Visualization Script -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  const stereoScenes = ['0001', '0006', '0048', '0071', '0150'];
  const stereoMethods = ['pred_gt', 'unimatch', 'prope', 'rayrope'];
  const methodContainerIds = {
    'pred_gt': 'pointcloud-gt',
    'unimatch': 'pointcloud-unimatch',
    'prope': 'pointcloud-prope',
    'rayrope': 'pointcloud-rayrope'
  };
  
  let currentStereoScene = '0001';

  // Common layout for all point clouds
  const commonLayout = {
    autosize: true,
    margin: { l: 0, r: 0, t: 0, b: 0 },
    scene: {
      aspectmode: 'data',
      xaxis: { visible: false },
      yaxis: { visible: false },
      zaxis: { visible: false },
      camera: {
        eye: { x: 0, y: 0, z: -2 }
      }
    },
    paper_bgcolor: 'rgba(0,0,0,0)',
    plot_bgcolor: 'rgba(0,0,0,0)'
  };

  const commonConfig = {
    responsive: true,
    displayModeBar: false
  };

  function decodeBdata(field) {
    if (!field || typeof field !== 'object' || !field.bdata || !field.dtype) return field;
    const binary = atob(field.bdata);
    const bytes = new Uint8Array(binary.length);
    for (let i = 0; i < binary.length; i++) {
      bytes[i] = binary.charCodeAt(i);
    }
    const buffer = bytes.buffer;
    switch (field.dtype) {
      case 'f4':
        return new Float32Array(buffer);
      case 'f8':
        return new Float64Array(buffer);
      case 'i4':
        return new Int32Array(buffer);
      case 'u1':
        return new Uint8Array(buffer);
      default:
        return field;
    }
  }

  // Function to load and display a point cloud
  async function loadPointCloud(scene, method) {
    const containerId = methodContainerIds[method];
    const container = document.getElementById(containerId);
    if (!container) return;

    const jsonPath = `./assets/stereo/${scene}_3d_pointcloud_${method}.json`;
    
    try {
      container.innerHTML = '';
      const response = await fetch(jsonPath);
      if (!response.ok) {
        throw new Error(`HTTP ${response.status}`);
      }
      const plotData = await response.json();

      if (plotData.data && Array.isArray(plotData.data)) {
        plotData.data = plotData.data.map((trace) => {
          const updated = { ...trace };
          updated.x = decodeBdata(trace.x);
          updated.y = decodeBdata(trace.y);
          updated.z = decodeBdata(trace.z);
          if (updated.marker && typeof updated.marker === 'object') {
            updated.marker = { ...updated.marker, size: 1 };
          }
          updated.hoverinfo = 'skip';
          updated.hovertemplate = '';
          updated.showlegend = false;
          return updated;
        });
      }
      
      if (!window.Plotly) {
        throw new Error('Plotly not loaded');
      }

      // Use the data from JSON but apply our common layout
      await Plotly.newPlot(containerId, plotData.data, {
        autosize: true,
        margin: { l: 0, r: 0, t: 0, b: 0 },
        scene: {
          aspectmode: 'data',
          xaxis: { visible: false },
          yaxis: { visible: false },
          zaxis: { visible: false },
          camera: {
            eye: { x: 0.8, y: 1.12, z: 0.08}
          }
        },
        hovermode: false,
        paper_bgcolor: 'rgba(0,0,0,0)',
        plot_bgcolor: 'rgba(0,0,0,0)'
      }, commonConfig);
    } catch (error) {
      console.error(`Error loading point cloud ${scene} ${method}:`, error);
      container.innerHTML = `<div style="display:flex;align-items:center;justify-content:center;height:100%;color:#999;">Failed to load</div>`;
    }
  }

  // Function to load all point clouds for a scene
  async function loadAllPointClouds(scene) {
    // Show loading state
    stereoMethods.forEach(method => {
      const container = document.getElementById(methodContainerIds[method]);
      if (container) {
        container.__cameraSyncBound = false;
        container.innerHTML = `<div style="display:flex;align-items:center;justify-content:center;height:100%;color:#999;">Loading...</div>`;
      }
    });

    // Load all point clouds in parallel
    await Promise.all(stereoMethods.map(method => loadPointCloud(scene, method)));
    wireCameraSync();
  }

  function wireCameraSync() {
    const containerIds = stereoMethods.map(method => methodContainerIds[method]);
    containerIds.forEach((sourceId) => {
      const sourceEl = document.getElementById(sourceId);
      if (!sourceEl) return;
      if (sourceEl.__cameraSyncBound) return;
      sourceEl.__cameraSyncBound = true;

      const syncCamera = (eventData) => {
        if (!eventData || !eventData['scene.camera']) return;
        if (sourceEl.__isSyncing) return;
        const camera = eventData['scene.camera'];
        containerIds.forEach((targetId) => {
          if (targetId === sourceId) return;
          const targetEl = document.getElementById(targetId);
          if (!targetEl) return;
          targetEl.__isSyncing = true;
          Plotly.relayout(targetId, { 'scene.camera': camera }).finally(() => {
            targetEl.__isSyncing = false;
          });
        });
      };

      sourceEl.on('plotly_relayouting', syncCamera);
      sourceEl.on('plotly_relayout', syncCamera);
    });
  }

  // Scene selector event listeners
  document.querySelectorAll('#stereo-scene-selector .scene-item').forEach(item => {
    item.addEventListener('click', () => {
      document.querySelectorAll('#stereo-scene-selector .scene-item').forEach(t => t.classList.remove('active'));
      item.classList.add('active');
      currentStereoScene = item.dataset.scene;
      loadAllPointClouds(currentStereoScene);
    });
  });

  function initStereoWhenVisible() {
    if ('IntersectionObserver' in window) {
      const section = document.getElementById('depth-estimation');
      const observer = new IntersectionObserver((entries) => {
        entries.forEach((entry) => {
          if (entry.isIntersecting) {
            loadAllPointClouds(currentStereoScene);
            observer.disconnect();
          }
        });
      }, { rootMargin: '200px 0px' });
      if (section) observer.observe(section);
    } else {
      loadAllPointClouds(currentStereoScene);
    }
  }

  // Initial load (lazy)
  initStereoWhenVisible();
});

// ========== Emergent Depth Scene Selection ==========
document.addEventListener('DOMContentLoaded', function() {
  const depthPredImg = document.getElementById('depth-pred-view');
  const depthErrorImg = document.getElementById('depth-error-view');
  const emergentDepthSceneSelector = document.getElementById('emergent-depth-scene-selector');
  const depthViewSelector = document.getElementById('depth-view-selector');
  const sceneSelectorLabel = emergentDepthSceneSelector ? emergentDepthSceneSelector.previousElementSibling : null;

  let currentDepthView = 'visualization';
  let currentDepthScene = '101_11750_20715_v0';

  // View selector (Visualization vs Accuracy Plot)
  if (depthViewSelector) {
    document.querySelectorAll('#depth-view-selector button').forEach(btn => {
      btn.addEventListener('click', () => {
        document.querySelectorAll('#depth-view-selector button').forEach(b => b.classList.remove('active'));
        btn.classList.add('active');
        currentDepthView = btn.dataset.view;

        if (currentDepthView === 'visualization') {
          depthPredImg.style.display = 'block';
          depthErrorImg.style.display = 'none';
          if (emergentDepthSceneSelector) emergentDepthSceneSelector.style.display = 'flex';
          if (sceneSelectorLabel) sceneSelectorLabel.style.display = 'block';
        } else {
          depthPredImg.style.display = 'none';
          depthErrorImg.style.display = 'block';
          if (emergentDepthSceneSelector) emergentDepthSceneSelector.style.display = 'none';
          if (sceneSelectorLabel) sceneSelectorLabel.style.display = 'none';
        }
      });
    });
  }

  if (!emergentDepthSceneSelector) return;

  // Scene selector event listeners for emergent depth
  document.querySelectorAll('#emergent-depth-scene-selector .scene-item').forEach(item => {
    item.addEventListener('click', () => {
      // Remove active class from all items
      document.querySelectorAll('#emergent-depth-scene-selector .scene-item').forEach(t => t.classList.remove('active'));
      
      // Add active class to clicked item
      item.classList.add('active');
      
      // Get the scene identifier
      currentDepthScene = item.dataset.scene;
      
      // Update depth visualization image
      if (depthPredImg) {
        depthPredImg.src = `./assets/depth_visual_final/${currentDepthScene}.png`;
      }
    });
  });
});
</script>

</body>
</html>
